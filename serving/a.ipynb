{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ad72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'mlflow'  \n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'mlflowpass' \n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://localhost:10000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c50fff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmj388/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Downloading artifacts: 100%|██████████| 6/6 [00:00<00:00, 15.16it/s]  \n",
      "2025/08/06 08:32:21 WARNING mlflow.pytorch: Stored model version '2.1.0' does not match installed PyTorch version '2.7.1+cu126'\n",
      "\n",
      "2025/08/06 08:32:21 WARNING mlflow.pytorch: Stored model version '2.1.0' does not match installed PyTorch version '2.7.1+cu126'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unet \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mruns:/d1a022812dcd4b0a99d693a7920d38e5/model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/iris-mlops/lib/python3.10/site-packages/mlflow/pytorch/__init__.py:657\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_uri, dst_path, **kwargs)\u001b[0m\n\u001b[1;32m    651\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    652\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStored model version \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not match installed PyTorch version \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    653\u001b[0m         pytorch_conf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_version\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    654\u001b[0m         torch\u001b[38;5;241m.\u001b[39m__version__,\n\u001b[1;32m    655\u001b[0m     )\n\u001b[1;32m    656\u001b[0m torch_model_artifacts_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(local_model_path, pytorch_conf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_data\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 657\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_model_artifacts_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/iris-mlops/lib/python3.10/site-packages/mlflow/pytorch/__init__.py:576\u001b[0m, in \u001b[0;36m_load_model\u001b[0;34m(path, device, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Version(torch\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.5.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 576\u001b[0m     pytorch_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;66;03m# load the model as an eager model.\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/iris-mlops/lib/python3.10/site-packages/torch/serialization.py:1525\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1524\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1525\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1533\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/micromamba/envs/iris-mlops/lib/python3.10/site-packages/torch/serialization.py:2114\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   2113\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 2114\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2115\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2117\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/micromamba/envs/iris-mlops/lib/python3.10/site-packages/torch/serialization.py:2103\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   2102\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 2103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unet'"
     ]
    }
   ],
   "source": [
    "unet = mlflow.pytorch.load_model(\"runs:/d1a022812dcd4b0a99d693a7920d38e5/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e6a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import mlflow\n",
    "import mlflow.pytorch  # ✅ NEW\n",
    "from dotenv import load_dotenv\n",
    "import argparse\n",
    "\n",
    "load_dotenv()\n",
    "job_name = os.getenv(\"name\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_dir\", type=str, default=\"Chest-X-Ray\", help=\"Data directory\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.001, help=\"Learning rate\")\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=5, help=\"Number of epochs\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# 하이퍼파라미터13213\n",
    "BATCH_SIZE = args.batch_size\n",
    "EPOCHS = args.num_epochs\n",
    "LR = args.lr\n",
    "DATA_DIR = \"/data/chest_xray\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ MLflow 설정\n",
    "mlflow.set_tracking_uri(\"http://10.125.208.187:5000\")\n",
    "mlflow.set_experiment(\"Lung-Xray-Classifier\")\n",
    "\n",
    "# 이미지 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 데이터셋\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(DATA_DIR, \"train\"), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=os.path.join(DATA_DIR, \"val\"), transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(DATA_DIR, \"test\"), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(\"Classes:\", train_dataset.classes)\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ✅ MLflow 실험 시작\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.set_tag(\"job_name\", job_name)\n",
    "    # 파라미터 로깅\n",
    "    mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "    mlflow.log_param(\"epochs\", EPOCHS)\n",
    "    mlflow.log_param(\"lr\", LR)\n",
    "    mlflow.log_param(\"model\", \"resnet18\")\n",
    "    mlflow.log_param(\"dataset\", \"chest_xray\")\n",
    "\n",
    "    # 학습\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {avg_loss:.4f}\")\n",
    "        mlflow.log_metric(\"train_loss\", avg_loss, step=epoch)\n",
    "\n",
    "    # 검증 함수\n",
    "    def evaluate(model, loader, name=\"val\"):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                outputs = model(x)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y).sum().item()\n",
    "        acc = 100 * correct / total\n",
    "        mlflow.log_metric(f\"{name}_accuracy\", acc)\n",
    "        print(f\"{name.capitalize()} Accuracy: {acc:.2f}%\")\n",
    "        return acc\n",
    "\n",
    "    evaluate(model, val_loader, \"val\")\n",
    "    evaluate(model, test_loader, \"test\")\n",
    "\n",
    "    # 모델 artifact 저장\n",
    "    mlflow.pytorch.log_model(model, artifact_path=\"model\")\n",
    "    print(f\"✅ 모델 저장 완료: Run ID = {run.info.run_id}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (iris-mlops)",
   "language": "python",
   "name": "iris-mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
